"""
RAG (Retrieval-Augmented Generation) benchmarking for GhostWire Refractory with GHOSTWIRE scoring
"""

import asyncio
import os
import sys
import time

import httpx
import psutil

# Add the python directory to the path to access ghostwire modules
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from python.ghostwire.config.settings import settings
from python.ghostwire.utils.ghostwire_scoring import (
    compute_general_ghostwire_score,
    compute_rag_ghostwire_score,
    format_benchmark_results_with_scores,
)


class RAGBenchmark:
    """Benchmark class for RAG performance testing"""

    def __init__(self, controller_url: str = None):
        self.controller_url = controller_url or "http://localhost:8000"
        self.client = httpx.AsyncClient(timeout=60.0)

    async def test_rag_query(
        self, session_id: str, query: str, model: str = None, iterations: int = 5
    ) -> tuple[float, list[float], float]:
        """Test RAG query performance and quality"""
        latencies = []
        memory_deltas = []
        model = model or settings.DEFAULT_OLLAMA_MODEL

        for i in range(iterations):
            # Record memory before
            memory_before = psutil.virtual_memory().used / (1024**3)  # GB

            start_time = time.perf_counter()
            try:
                # First get embedding for the query
                embedding_response = await self.client.post(
                    f"{self.controller_url}/api/v1/embeddings",
                    json={"input": query, "model": "nomic-embed-text"},
                )
                embedding_response.raise_for_status()
                embedding_data = embedding_response.json()
                embedding = embedding_data["data"][0]["embedding"]

                # Then make the RAG query
                rag_response = await self.client.post(
                    f"{self.controller_url}/api/v1/chat/chat_embedding",
                    json={
                        "session_id": session_id,
                        "text": query,
                        "embedding": embedding,
                    },
                )
                rag_response.raise_for_status()

                latency = time.perf_counter() - start_time

                # Record memory after
                memory_after = psutil.virtual_memory().used / (1024**3)  # GB
                memory_delta = memory_after - memory_before

                latencies.append(latency)
                memory_deltas.append(memory_delta)
            except Exception as e:
                print(f"RAG query failed: {e}")
                continue

        avg_latency = sum(latencies) / len(latencies) if latencies else float("inf")
        avg_memory_delta = (
            sum(memory_deltas) / len(memory_deltas) if memory_deltas else 0.0
        )

        # For quality assessment, we would typically need to evaluate the response
        # For now, we'll use a placeholder quality score and hallucination rate
        # In a real implementation, we would implement response quality checks
        quality = 0.75  # Placeholder quality score
        hallucination_rate = 0.15  # Placeholder hallucination rate (15%)

        return avg_latency, latencies, avg_memory_delta, quality, hallucination_rate

    async def test_retrieval_only(
        self, session_id: str, query: str, iterations: int = 5
    ) -> tuple[float, list[float], float]:
        """Test retrieval-only performance (without generation)"""
        latencies = []
        memory_deltas = []

        for i in range(iterations):
            # Record memory before
            memory_before = psutil.virtual_memory().used / (1024**3)  # GB

            start_time = time.perf_counter()
            try:
                # Get embedding for the query
                embedding_response = await self.client.post(
                    f"{self.controller_url}/api/v1/embeddings",
                    json={"input": query, "model": "nomic-embed-text"},
                )
                embedding_response.raise_for_status()
                embedding_data = embedding_response.json()
                embedding = embedding_data["data"][0]["embedding"]

                # Query vectors
                rag_response = await self.client.post(
                    f"{self.controller_url}/api/v1/vectors/query",
                    json={"namespace": session_id, "embedding": embedding, "top_k": 5},
                )
                rag_response.raise_for_status()

                latency = time.perf_counter() - start_time

                # Record memory after
                memory_after = psutil.virtual_memory().used / (1024**3)  # GB
                memory_delta = memory_after - memory_before

                latencies.append(latency)
                memory_deltas.append(memory_delta)
            except Exception as e:
                print(f"Retrieval query failed: {e}")
                continue

        avg_latency = sum(latencies) / len(latencies) if latencies else float("inf")
        avg_memory_delta = (
            sum(memory_deltas) / len(memory_deltas) if memory_deltas else 0.0
        )

        # For retrieval quality, we would typically evaluate the relevance of retrieved documents
        # For now, we'll use a placeholder
        retrieval_quality = 0.80  # Placeholder retrieval quality score
        consistency = 0.85  # Placeholder consistency score

        return avg_latency, latencies, avg_memory_delta, retrieval_quality, consistency

    async def run_rag_benchmark(self):
        """Run the complete RAG benchmark suite"""
        print("üöÄ Starting RAG Benchmark Suite")
        print("=" * 60)

        test_queries = [
            "What is quantum computing?",
            "Explain the theory of relativity",
            "How does photosynthesis work?",
            "What are neural networks?",
            "Explain blockchain technology",
        ]

        session_id = "rag_benchmark_session"

        # Test retrieval-only performance
        print("\nüîç Testing Retrieval Performance...")
        (
            avg_retrieval_lat,
            _,
            retrieval_memory,
            retrieval_quality,
            consistency,
        ) = await self.test_retrieval_only(session_id, test_queries[0], iterations=3)
        print(f"  Average retrieval latency: {avg_retrieval_lat:.4f}s")
        print(f"  Average retrieval memory usage: {retrieval_memory:.4f} GB")
        print(f"  Retrieval quality: {retrieval_quality:.4f}")
        print(f"  Retrieval consistency: {consistency:.4f}")

        # Test full RAG performance
        print("\nü§ñ Testing Full RAG Performance...")
        (
            avg_rag_lat,
            _,
            rag_memory,
            rag_quality,
            hallucination_rate,
        ) = await self.test_rag_query(session_id, test_queries[0], iterations=3)
        print(f"  Average RAG latency: {avg_rag_lat:.4f}s")
        print(f"  Average RAG memory usage: {rag_memory:.4f} GB")
        print(f"  RAG quality: {rag_quality:.4f}")
        print(f"  Hallucination rate: {hallucination_rate:.4f}")

        print("\n‚úÖ RAG benchmark suite completed!")
        print("=" * 60)

        # Calculate GHOSTWIRE scores
        retrieval_ghostwire_score = compute_general_ghostwire_score(
            latency=avg_retrieval_lat,
            stability=consistency,
            memory_usage=retrieval_memory,
        )

        rag_ghostwire_score = compute_rag_ghostwire_score(
            quality=rag_quality, hallucination=hallucination_rate, latency=avg_rag_lat
        )

        overall_ghostwire_score = (retrieval_ghostwire_score + rag_ghostwire_score) / 2

        print("\nüèÜ GHOSTWIRE SCORES:")
        print(f"  Retrieval Performance Score: {retrieval_ghostwire_score:.4f}")
        print(f"  RAG Performance Score: {rag_ghostwire_score:.4f}")
        print(f"  Overall RAG GHOSTWIRE Score: {overall_ghostwire_score:.4f}")

        return {
            "retrieval_latency": avg_retrieval_lat,
            "retrieval_memory": retrieval_memory,
            "retrieval_quality": retrieval_quality,
            "retrieval_consistency": consistency,
            "rag_latency": avg_rag_lat,
            "rag_memory": rag_memory,
            "rag_quality": rag_quality,
            "hallucination_rate": hallucination_rate,
            "retrieval_ghostwire_score": retrieval_ghostwire_score,
            "rag_ghostwire_score": rag_ghostwire_score,
            "overall_ghostwire_score": overall_ghostwire_score,
        }


async def main():
    benchmark = RAGBenchmark()
    results = await benchmark.run_rag_benchmark()

    print("\nüìà RAG BENCHMARK RESULTS:")
    print(f"  Retrieval Latency: {results['retrieval_latency']:.4f}s")
    print(f"  Retrieval Memory Usage: {results['retrieval_memory']:.4f} GB")
    print(f"  Retrieval Quality: {results['retrieval_quality']:.4f}")
    print(f"  Retrieval Consistency: {results['retrieval_consistency']:.4f}")
    print(f"  RAG Latency: {results['rag_latency']:.4f}s")
    print(f"  RAG Memory Usage: {results['rag_memory']:.4f} GB")
    print(f"  RAG Quality: {results['rag_quality']:.4f}")
    print(f"  Hallucination Rate: {results['hallucination_rate']:.4f}")
    print(f"  Retrieval GHOSTWIRE Score: {results['retrieval_ghostwire_score']:.4f}")
    print(f"  RAG GHOSTWIRE Score: {results['rag_ghostwire_score']:.4f}")
    print(f"  Overall GHOSTWIRE Score: {results['overall_ghostwire_score']:.4f}")

    # Also print formatted results with GHOSTWIRE scoring
    print(
        "\n" + format_benchmark_results_with_scores(results, score_function_name="rag")
    )


if __name__ == "__main__":
    asyncio.run(main())
